{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c5d08680","cell_type":"markdown","source":"# **Reti Neurali** ","metadata":{}},{"id":"52a5f30f","cell_type":"markdown","source":"## **Esercizio 1: Download e pre-processamento dei dati.**\n\nScaricare e pre-processare i dati per il successivo addestramento del modello. \n\nIl dataset che utilizzeremo sarà CIFAR10, scaricabile dalla libreria `tensorflow.keras.datasets`","metadata":{}},{"id":"90f97f6b-2a97-4bb4-9d71-c001c4dbceae","cell_type":"code","source":"!pip install tensorflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T06:53:01.059273Z","iopub.execute_input":"2025-05-27T06:53:01.059522Z","iopub.status.idle":"2025-05-27T06:53:07.404986Z","shell.execute_reply.started":"2025-05-27T06:53:01.059501Z","shell.execute_reply":"2025-05-27T06:53:07.403468Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.0rc1)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","output_type":"stream"}],"execution_count":1},{"id":"42e9791d","cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.datasets import cifar10\n\n# Download dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\ny_train = y_train.ravel()\ny_test = y_test.ravel()\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T06:53:38.752248Z","iopub.execute_input":"2025-05-27T06:53:38.752649Z","iopub.status.idle":"2025-05-27T06:54:05.173900Z","shell.execute_reply.started":"2025-05-27T06:53:38.752614Z","shell.execute_reply":"2025-05-27T06:54:05.173048Z"}},"outputs":[{"name":"stderr","text":"2025-05-27 06:53:40.966499: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748328821.252358      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748328821.334349      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n(50000, 32, 32, 3)\n(50000,)\n","output_type":"stream"}],"execution_count":2},{"id":"4d6a0ca6","cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Pre-processamento dei dati\n\nx_train_flat = x_train.reshape((x_train.shape[0], -1))\nx_test_flat = x_test.reshape((x_test.shape[0], -1))\n\nscaler = StandardScaler()\nx_train_scaled = scaler.fit_transform(x_train_flat)\nx_test_scaled = scaler.fit_transform(x_test_flat)\n\nprint(\"train\", x_train_scaled.shape)\nprint(\"test\", x_test_scaled.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:03:00.202546Z","iopub.execute_input":"2025-05-27T07:03:00.204285Z","iopub.status.idle":"2025-05-27T07:03:05.094503Z","shell.execute_reply.started":"2025-05-27T07:03:00.204231Z","shell.execute_reply":"2025-05-27T07:03:05.093304Z"}},"outputs":[{"name":"stdout","text":"train (50000, 3072)\ntest (10000, 3072)\n","output_type":"stream"}],"execution_count":6},{"id":"5103ad7f","cell_type":"markdown","source":"## **Esercizio 2: Creare modello MLP**\n\nPer creare il modello MLP utilizziamo l' oggetto `MLPClassifier` dal modulo `sklearn.neural_networks`. Questo è un oggetto molto complesso che prevede la possibilità di specificare tanti parametri, permettendoci una personalizzazione molto dettagliata. Vediamo di seguito gli argomenti principali:\n\n- `hidden_layer_sizes`: rappresenta la struttura dell' MLP, sotto forma di una tupla. La tupla deve essere composta da numeri interi, ogni numero indica il numero di neuroni presenti nel rispettivo layer.\n\nEsempio: \n\n`hidden_layer_sizes` = `(100)`\n\ncreerà un solo layer con 100 neuroni\n\n`hidden_layer_sizes` = `(100, 50)`\n\ncreerà due layer, il primo con 100 neuroni, il secondo invece con 50.\n\n- `max_iter`: massimo numero di iterazioni per raggiungere la convergenza. \n\n- `activation`: indica quale funzione di attivazione utilizzare, valori possibili sono `'relu'`, `'logistic'`, `'tanh'` and `'identity'`.\n\n- `solver`: indica quale algoritmo di ottimizzazione utilizzare, valori possibili sono `'adam'`, `'sgd'` and `'lbfgs'`.\n\n- `learning_rate_init`: valore iniziale del learning rate.\n\n- `verbose`: valore booleano che, se impostato su `True`, stampa l' output di ogni iterazione di training. Molto utile per monitorare il training.\n\n- `random_state`: fissa il seed della randomizzazione.\n","metadata":{}},{"id":"b0f857d0","cell_type":"markdown","source":"Per iniziare creiamo un MLP molto basilare, alleniamolo e testiamone le performance. Come parametri utilizzeremo:\n\n- `hidden_layer_sizes` = `(100)`\n\n- `max_iter` = `20`\n\n- `random_state` = `42`","metadata":{}},{"id":"4650894a","cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Creare MLP \n\nmlp = MLPClassifier(\n    hidden_layer_sizes=(100),\n    max_iter=20, \n    activation = 'relu' ,\n    random_state=42 ,\n    )\n\n\n# Allenare MLP\n\nmlp.fit(x_train_scaled, y_train)\n\n\n# Valutare MLP\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuratezza:\", accuracy)\n\n\n\n# Stampare l' accuratezza\n\nprint(\"Accuratezza:\", accuracy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:11:31.214442Z","iopub.execute_input":"2025-05-27T07:11:31.214952Z","iopub.status.idle":"2025-05-27T07:12:33.442382Z","shell.execute_reply.started":"2025-05-27T07:11:31.214921Z","shell.execute_reply":"2025-05-27T07:12:33.441132Z"}},"outputs":[{"name":"stdout","text":"Iteration 1, loss = 1.84922213\nIteration 2, loss = 1.52410225\nIteration 3, loss = 1.40419682\nIteration 4, loss = 1.33204438\nIteration 5, loss = 1.27721092\nIteration 6, loss = 1.24043705\nIteration 7, loss = 1.20071293\nIteration 8, loss = 1.16975351\nIteration 9, loss = 1.14043570\nIteration 10, loss = 1.11965949\nIteration 11, loss = 1.09338560\nIteration 12, loss = 1.07412903\nIteration 13, loss = 1.05735050\nIteration 14, loss = 1.03938088\nIteration 15, loss = 1.02135505\nIteration 16, loss = 0.99397657\nIteration 17, loss = 0.97903139\nIteration 18, loss = 0.96303503\nIteration 19, loss = 0.95288051\nIteration 20, loss = 0.92579253\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"MLPClassifier(hidden_layer_sizes=100, max_iter=20, random_state=42,\n              verbose=True)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=100, max_iter=20, random_state=42,\n              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=100, max_iter=20, random_state=42,\n              verbose=True)</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":9},{"id":"6171280e","cell_type":"markdown","source":"## **Esercizio 2.1: Aumentiamo i parametri del nostro modello**\n\nProviamo adesso ad aumentare i dettagli del nostro modello, modificando o aggiungendo i parametri sopra specificati. ","metadata":{}},{"id":"f35afa68","cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Creare MLP con più strati e altre specifiche\n\nmlp = MLPClassifier(\n    hidden_layer_sizes=(100),\n    max_iter=20, \n    activation = 'relu' ,\n    solver = 'adam' , \n    learning_rate_init = 0.001 , \n    verbose = True , \n    random_state=42 ,\n    )\n\n\n# Allenare MLP\n\nmlp.fit(x_train_scaled, y_train)\n\n\n# Valutare MLP\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuratezza:\", accuracy)\n\n\n\n# Stampare l' accuratezza\n\nprint(\"Accuratezza:\", accuracy)\n","metadata":{},"outputs":[],"execution_count":null},{"id":"226b1681","cell_type":"markdown","source":"## **Esercizio 3: Implementare manualmente l' algoritmo di early stopping.**\n\nL' algoritmo di early stopping ci permette di terminare anticipatamente l' allenamento di un modello nel caso in cui questo raggiunga la convergenza. Supponiamo infatti che il nostro modello raggiunga un certo livello di accuratezza e che non riesca a migliorare oltre quel livello. Questo significa che il modello, da quel momento in poi, non sta più apprendendo nuove informazioni, per cui le successive iterazioni sono superflue, ed inoltre rischiano di essere dannose, spingendo il modello verso l' overfitting. \n\nL' early stopping verifica ad ogni iterazione che l' accuratezza del modello sia incrementata di una certa tolleranza. Se questa tolleranza non viene superata per un certo numero di epoche, allora possiamo decidere di stoppare l' allenamento in quanto il modello ha raggiunto la convergenza.\n\n**N.B: per applicare early stopping è necessario specificare i seguenti parametri dell' MLP:**\n\n- `warm_start`=`True` in modo che il training proceda dallo stato attuale del modello e non dall' inizializzazione.\n\n- `max_iter`=`1` in modo che il modello venga allenato per una sola epoca. Per l' early stopping infatti dovremo gestire manualmente il numero di iterazioni.","metadata":{}},{"id":"8ee1709d","cell_type":"code","source":"import numpy as np\nfrom sklearn.neural_network import MLPClassifier \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.model_selection import train_test_split\n\nx_train_partial, x_val, y_train_partial, y_val = train_test_split(x_train_scaled, y_train, test_size = 0.2, random_state = 42)\nn_total_epochs = 100  \npatience = 10         \ntolerance = 1e-4      \n\nbest_test_accuracy = 0.0\nepochs_without_improvement = 0\n\nmlp = MLPClassifier(\n    hidden_layer_sizes=(100),\n    max_iter=1,\n    warm_start=True,\n    random_state=42\n)\nval_accuracies = []\n\nfor epoch in range(n_total_epochs):\n    mlp.fit(x_train_partial, y_train_partial)\n    \n    y_val_pred = mlp.predict(x_val)\n    val_accuracy = accuracy_score(y_val, y_val_pred)\n    val_accuracies.append(val_accuracy)\n\n    print(f\"Epoca {epoch + 1}, Accuratezza validazione: {val_accuracy:.4f}\")\n\n    if val_accuracy > best_test_accuracy + tolerance:\n        best_test_accuracy = val_accuracy\n        epochs_without_improvement = 0\n    else:\n        epochs_without_improvement += 1\n\n    if epochs_without_improvement >= patience:\n        print(f\"Early stopping attivato dopo {epoch + 1} epoche. Migliore accuratezza validazione: {best_test_accuracy:.4f}\")\n        break\n\ny_test_pred = mlp.predict(x_test_scaled)\ntest_accuracy= accuracy_score(y_test, y_test_pred)\nprint(f\"Accuratezza finale sul test set: {test_accuracy:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:42:07.954487Z","iopub.execute_input":"2025-05-27T07:42:07.955105Z","iopub.status.idle":"2025-05-27T07:43:18.960350Z","shell.execute_reply.started":"2025-05-27T07:42:07.955072Z","shell.execute_reply":"2025-05-27T07:43:18.959258Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoca 1, Accuratezza validazione: 0.4290\nEpoca 2, Accuratezza validazione: 0.4517\nEpoca 3, Accuratezza validazione: 0.4555\nEpoca 4, Accuratezza validazione: 0.4604\nEpoca 5, Accuratezza validazione: 0.4681\nEpoca 6, Accuratezza validazione: 0.4732\nEpoca 7, Accuratezza validazione: 0.4734\nEpoca 8, Accuratezza validazione: 0.4735\nEpoca 9, Accuratezza validazione: 0.4774\nEpoca 10, Accuratezza validazione: 0.4781\nEpoca 11, Accuratezza validazione: 0.4783\nEpoca 12, Accuratezza validazione: 0.4772\nEpoca 13, Accuratezza validazione: 0.4781\nEpoca 14, Accuratezza validazione: 0.4757\nEpoca 15, Accuratezza validazione: 0.4786\nEpoca 16, Accuratezza validazione: 0.4766\nEpoca 17, Accuratezza validazione: 0.4743\nEpoca 18, Accuratezza validazione: 0.4742\nEpoca 19, Accuratezza validazione: 0.4696\nEpoca 20, Accuratezza validazione: 0.4685\nEpoca 21, Accuratezza validazione: 0.4672\nEpoca 22, Accuratezza validazione: 0.4659\nEpoca 23, Accuratezza validazione: 0.4621\nEpoca 24, Accuratezza validazione: 0.4620\nEpoca 25, Accuratezza validazione: 0.4604\nEarly stopping attivato dopo 25 epoche. Migliore accuratezza validazione: 0.4786\nAccuratezza finale sul test set: 0.4594\n","output_type":"stream"}],"execution_count":16},{"id":"9ddace1e","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}